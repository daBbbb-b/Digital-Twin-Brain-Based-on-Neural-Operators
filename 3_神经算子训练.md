
## **成员 C 的任务说明：神经算子反演训练**

### **任务目标**

成员 C 负责基于**成员 A 构建的仿真数据集**，训练神经算子模型（如 Fourier Neural Operator, FNO 或 DeepONet），**学习从大脑活动数据（BOLD / 神经状态）到潜在刺激函数的反演映射**。

具体而言，神经算子需要近似以下**反问题算子**：

$$
\mathcal{G}: x(t) \longrightarrow u(t)
$$

其中：

* \( u(t) \)：任务相关的潜在刺激函数（驱动刺激 + 调制刺激）
* \( x(t) \)：由 ODE / PDE 神经动力学模型生成的神经状态或 BOLD 信号

最终目标是将该反演算子**泛化应用到真实 103-task fMRI 数据**中，推断任务刺激图谱，并分析其与真实功能图谱之间的一致性。

---

## **1. 数据准备与理解**

### **仿真数据来源（由成员 A 提供）**

仿真数据集由已知刺激 (u(t)) 经正向动力学模型生成，用于监督反演算子的训练，包括：

* **PDE 数据集**

  * 基于皮层 surface 与结构连接
* **ODE-EC 数据集**

  * 基于有效连接（Effective Connectivity）
* **ODE-SC 数据集**

  * 基于白质结构连接（Structural Connectivity）

### **每个样本包含：**

* **刺激函数 (u(t))**
  任务驱动刺激与调制刺激的真实生成函数（监督信号）
* **神经状态 / BOLD 信号 (x(t))**
  正向动力学模型的输出（神经算子输入）
* **连接矩阵 (W)**
  EC / SC / 皮层结构连接，用作条件信息或模型输入

---

### **输入 / 输出角色（非常关键）**

| 角色  | 数学对象   | 在神经算子中的地位      |
| --- | ------ | -------------- |
| 观测量 | (x(t)) | **输入函数**       |
| 潜变量 | (u(t)) | **输出函数（反演目标）** |

---

## **2. 神经算子模型选择**

成员 C 可选择以下模型之一：

* **Fourier Neural Operator (FNO)**
* **DeepONet**

两者均用于学习**函数到函数的映射**，在此任务中具体为：

$$
x(t) \Rightarrow u(t)
$$

### **模型选择建议**

* **FNO**

  * 适合规则时间轴、全局频谱结构明显的任务
  * 能捕捉任务引起的长程时间依赖
* **DeepONet**

  * 更适合处理不规则时间采样或条件变量较多的情形
  * 易于引入结构连接 (W) 作为条件输入

---

## **3. 神经算子反演训练**

### **数据组织方式**

每个训练样本由一对函数构成：

* **输入**：
  $$
  x(t) \in \mathbb{R}^{N \times T}
  $$
  表示 \( N \) 个脑区在 \( T \) 个时间点上的神经活动或 BOLD 信号

* **输出（监督信号）**：
  $$
  u(t) \in \mathbb{R}^{N \times T}
  $$
  表示对应的刺激函数（驱动 + 调制）

---

### **训练目标**

通过最小化反演误差，学习近似反算子：

$$
\mathcal{G}_\theta \approx \mathcal{F}^{-1}
$$

其中 \( \mathcal{F} \) 是未知但在仿真中可控的正向动力学算子。

---

### **损失函数**

* 基础损失：

  * 均方误差（MSE）
* 可选正则项（加分项）：

  * 时间平滑正则
  * 稀疏性约束（刺激应局部激活）
  * task-onset 对齐约束（与 events.tsv 一致）

---

### **FNO 反演训练伪代码（修订）**

```python
# x: 神经状态 / BOLD (输入)
# u: 刺激函数 (监督标签)

model = FNO(input_dim=246, output_dim=246)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    u_pred = model(x)          # x(t) -> û(t)
    loss = mse_loss(u_pred, u) # 反演损失

    loss.backward()
    optimizer.step()
```

---

## **4. 验证与泛化评估**

### **4.1 仿真数据集上的验证**

目的：验证神经算子是否正确学到反演映射。

* 输入：仿真生成的 (x_{\text{sim}}(t))
* 输出：预测刺激 (\hat{u}(t))
* 对比：

  * (\hat{u}(t)) vs (u_{\text{sim}}(t))
  * 时间对齐、空间分布、强度一致性

---

### **4.2 真实 103-task 数据上的应用**

真实数据中 **不存在真实刺激函数标签**，因此评估方式为**弱监督分析**：

* 输入：真实 fMRI BOLD（按 task / run 划分）
* 输出：推断刺激函数 (\hat{u}(t))
* 分析：

  * 不同 task 的刺激空间分布差异
  * 刺激图谱与功能激活 / FC / EC 的一致性
  * 同一 task 跨 run 的稳定性

---

## **5. 输出成果要求**

### **5.1 模型输出**

* 训练完成的神经算子模型（FNO / DeepONet）
* 支持从 (x(t)) → (\hat{u}(t)) 的反演推断

---

### **5.2 分析与报告**

* 仿真数据反演精度分析
* 不同动力学设置下的鲁棒性比较
* 真实 103-task 刺激图谱可视化
* 与真实功能图谱的一致性分析
* 失败案例与局限性讨论（非常重要）

---

